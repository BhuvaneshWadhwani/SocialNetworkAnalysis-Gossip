---
title: "BhuvaneshWadhwani_A0199148M_FinalProject"
author: "Bhuvanesh Wadhwani"
date: "2022-10-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval =TRUE, size = "\\tiny")
```

# Calling packages 
```{r Packages, cache = TRUE}

packages_to_use<- c("ROCR", "tidyverse", "caret", "glmnet", "rpart", "rpart.plot", "vip", "pdp", "randomForest", "gbm", "GGally", "cowplot", "dplyr", "DALEX", "DALEXtra", "lime", "localModel")

for(i in packages_to_use){
  if( ! i %in% rownames(installed.packages())  ) {
    print(paste(i, "not installed; installing now:\n") )
    install.packages(i)
  }
  
  require(i, character.only = TRUE)
}


```


# Part 1: Data Exploration
# Taking in data
```{r Data, cache = TRUE}
sact.dat <- read.csv("sac.csv")

#combining variables
sact.dat <- sact.dat %>%
  mutate(paredu = (Medu + Fedu) ) %>% 
  mutate(grades = (G1 + G2 + G3)/3)

head(sact.dat)
psych::describe(sact.dat)
#selecting columns of interest based on low skewness
sapply(sact.dat, var)

sacf.dat <- select(sact.dat, Mjob, Fjob, guardian, famsup, activities, romantic, freetime, health, goout, paredu, sex, age, reason, grades)

print(head(sacf.dat))
print(str(sacf.dat))

```
##--Details about the dataset: Student Alcohol Consumption--##

#school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)

#sex - student's sex (binary: 'F' - female or 'M' - male)

#age - student's age (numeric: from 15 to 22)

#address - student's home address type (binary: 'U' - urban or 'R' - rural)

#famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)

#Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)

#Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)

#Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)

#Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')

#Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')

#reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')

#guardian - student's guardian (nominal: 'mother', 'father' or 'other')

#traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)

#studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)

#failures - number of past class failures (numeric: n if 1<=n<3, else 4)

#schoolsup - extra educational support (binary: yes or no)

#famsup - family educational support (binary: yes or no)

#paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)

#activities - extra-curricular activities (binary: yes or no)

#nursery - attended nursery school (binary: yes or no)

#higher - wants to take higher education (binary: yes or no)

#internet - Internet access at home (binary: yes or no)

#romantic - with a romantic relationship (binary: yes or no)

#famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

#freetime - free time after school (numeric: from 1 - very low to 5 - very high)

#goout - going out with friends (numeric: from 1 - very low to 5 - very high)

#Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

#Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

#health - current health status (numeric: from 1 - very bad to 5 - very good)

#absences - number of school absences (numeric: from 0 to 93)

#G1 - first period grade (numeric: from 0 to 20)

#G2 - second period grade (numeric: from 0 to 20)

#G3 - final grade (numeric: from 0 to 20, output target)

##added variables:

#paredu - parent's education out of 8 (higher = higher combined education levels of both parents)

#grade - mean grade (numeric: from 0 to 20)





# Factoring variables in the dataset
```{r Factor, cache = TRUE}
#sex
sacf.dat$sex <- factor(sacf.dat$sex,
                       levels = c("F", "M"),
                       labels = c("Female", "Male"))

#Mjob
sacf.dat$Mjob <- factor(sacf.dat$Mjob,
                        levels = c("teacher", "health","services","at_home","other"),
                        labels = c("teacher", "health","services","at_home","other"))

#Fjob
sacf.dat$Fjob <- factor(sacf.dat$Fjob,
                        levels = c("teacher", "health","services","at_home","other"),
                        labels = c("teacher", "health","services","at_home","other"))

#guardian
sacf.dat$guardian <- factor(sacf.dat$guardian,
                            levels = c("mother", "father","other"),
                            labels = c("mother", "father","other"))


#famsup
sacf.dat$famsup <- factor(sacf.dat$famsup,
                          levels = c("no", "yes"),
                          labels = c("no", "yes"))


#activities
sacf.dat$activities <- factor(sacf.dat$activities,
                              levels = c("no", "yes"),
                              labels = c("no", "yes"))

#romantic
sacf.dat$romantic <- factor(sacf.dat$romantic,
                            levels = c("no", "yes"),
                            labels = c("no", "yes"))

#freetime
sacf.dat$freetime <- factor(sacf.dat$freetime,
                            levels = c("1","2","3","4","5"), 
                            labels = c("1","2","3","4","5"))

#goout
sacf.dat$goout <- factor(sacf.dat$goout,
                         levels = c("1","2","3","4", "5"), 
                         labels = c("1","2","3","4", "5"))

#reason
sacf.dat$reason <- factor(sacf.dat$reason,
                          levels = c("home", "reputation","course","other"),
                          labels = c("home", "reputation","course","other"))

#health
sacf.dat$health <- factor(sacf.dat$health,
                       levels = c("1","2","3","4","5"), 
                       labels = c("1","2","3","4","5"))



```



```{r Check Dataframe, cache = TRUE}

head(sacf.dat)
str(sacf.dat)
summary(sacf.dat)
psych::describe(sacf.dat) #skew is high for guardian, Mjob, Fjob. 
sum(is.na(sacf.dat)) #there are no missing values in the dataframe


```



```{r GGpairs, fig.height = 15, fig.width = 15, cache = TRUE}
#ggpairs to look at overall patterns
plotgg1 <- GGally::ggpairs(sacf.dat, 
                          progress = FALSE, 
                          alpha = 0.2)

suppressWarnings(print(plotgg1))
 

```

```{r count, cache = TRUE}

age.count <- ggplot(sacf.dat, aes(x = age, fill = age, color = age  ) ) +
  geom_histogram(alpha = 0.5, bins = 10) +
  theme(legend.position = "right")

paredu.count <- ggplot(sacf.dat, aes(x = paredu, fill = paredu, color = paredu  ) ) +
  geom_histogram(alpha = 0.5, bins = 10) +
  theme(legend.position = "right")


plot_grid(age.count, paredu.count, labels=c( "age", "paredu"), ncol = 2, nrow = 1)


```



```{r Scatterplots, fig.height = 9, fig.width = 10, cache = TRUE}

num.dat <- select_if(sacf.dat, is.numeric)
head(num.dat)

age<- ggplot(sacf.dat, aes(x = grades, y = age)) +
  geom_point(position = position_jitter(0.33), col = "red", alpha=0.3) + 
  geom_smooth(method = lm, se = FALSE,linetype ="dashed") +
  #facet_wrap(~sex) +
  theme(legend.position = "right")

paredu<- ggplot(sacf.dat, aes(x = grades, y = paredu )) +
  geom_point(position = position_jitter(0.33), col = "green", alpha=0.3) + 
  geom_smooth(method = lm, se = FALSE,linetype ="dashed") +
  #facet_wrap(~sex) +
  theme(legend.position = "right")

plot_grid(age, paredu,labels=c("age", "paredu"), ncol = 2, nrow = 1)

#the spread seems to be quite even, with a few outliers

```




```{r Histogram, fig.height = 14, fig.width = 14, cache = TRUE}

cat.dat <- select_if(sacf.dat, is.factor)
head(cat.dat)

#Histograms for each variables against grades
sex.plot <- ggplot(sacf.dat, aes(x = grades, fill = sex, color = sex  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

Mjob.plot <- ggplot(sacf.dat, aes(x = grades, fill = Mjob, color = Mjob  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

Fjob.plot <- ggplot(sacf.dat, aes(x = grades, fill = Fjob, color = Fjob  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

guardian.plot <- ggplot(sacf.dat, aes(x = grades, fill = guardian, color = guardian  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

famsup.plot <- ggplot(sacf.dat, aes(x = grades, fill = famsup, color = famsup  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

activities.plot <- ggplot(sacf.dat, aes(x = grades, fill = activities, color = activities  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

romantic.plot <- ggplot(sacf.dat, aes(x = grades, fill = romantic, color = romantic  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

freetime.plot <- ggplot(sacf.dat, aes(x = grades, fill = freetime, color = freetime  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

health.plot <- ggplot(sacf.dat, aes(x = grades, fill = health, color = health  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

goout.plot <- ggplot(sacf.dat, aes(x = grades, fill = goout, color = goout  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

reason.plot <- ggplot(sacf.dat, aes(x = grades, fill = reason, color = reason  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

age.plot <- ggplot(sacf.dat, aes(x = grades, fill = age, color = age  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

paredu.plot <- ggplot(sacf.dat, aes(x = grades, fill = paredu, color = paredu  ) ) +
  geom_histogram(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")


plot_grid(sex.plot, Mjob.plot, Fjob.plot, guardian.plot, famsup.plot, activities.plot, romantic.plot, freetime.plot, health.plot, goout.plot, reason.plot, age.plot, paredu.plot, 
          labels=c("sex", "Mjob","Fjob", "guardian", "famsup", "activities", "romantic", "freetime", "health", "goout",  "reason", "age",  "paredu"), 
          ncol = 3, nrow = 5)







```

```{r Bar Check, fig.height = 12, fig.width = 12, cache = TRUE}

sex.check <- ggplot(sacf.dat, aes(x = sex, fill = sex, color = sex  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

Mjob.check <- ggplot(sacf.dat, aes(x = Mjob, fill = Mjob, color = Mjob  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

Fjob.check <- ggplot(sacf.dat, aes(x = Fjob, fill = Fjob, color = Fjob  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

guardian.check <- ggplot(sacf.dat, aes(x = guardian, fill = guardian, color = guardian  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

famsup.check <- ggplot(sacf.dat, aes(x = famsup, fill = famsup, color = famsup  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

activities.check <- ggplot(sacf.dat, aes(x = activities, fill = activities, color = activities  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

romantic.check <- ggplot(sacf.dat, aes(x = romantic, fill = romantic, color = romantic  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

freetime.check <- ggplot(sacf.dat, aes(x = freetime, fill = freetime, color = freetime  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

health.check <- ggplot(sacf.dat, aes(x = health, fill = health, color = health  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

goout.check <- ggplot(sacf.dat, aes(x = goout, fill = goout, color = goout  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

reason.check <- ggplot(sacf.dat, aes(x = reason, fill = reason, color = reason  ) ) +
  geom_bar(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

plot_grid(sex.check, Mjob.check, Fjob.check, guardian.check,  famsup.check, activities.check, romantic.check, freetime.check, health.check, goout.check, reason.check, 
          labels=c("sex", "Mjob","Fjob", "guardian", "famsup", "activities", "romantic", "freetime", "health", "goout", "reason"), 
          ncol = 3, nrow = 4)

#again we see how Mjob, Fjob, and guardian are skewed.
#health does look a little skewed, but the psych::description showed that it is fine, so we will trust that.

```






```{r Boxplot, fig.height = 12, fig.width = 12, cache = TRUE}
#boxplots 
sex.plot1 <- ggplot(sacf.dat, aes(y = grades, x = sex, fill = sex, color = sex  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")


Mjob.plot1 <- ggplot(sacf.dat, aes(y = grades, x = Mjob, fill = Mjob, color = Mjob  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

Fjob.plot1 <- ggplot(sacf.dat, aes(y = grades, x = Fjob, fill = Fjob, color = Fjob  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

guardian.plot1 <- ggplot(sacf.dat, aes(y = grades, x = guardian, fill = guardian, color = guardian  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")


famsup.plot1 <- ggplot(sacf.dat, aes(y = grades, x = famsup, fill = famsup, color = famsup  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

activities.plot1 <- ggplot(sacf.dat, aes(y = grades, x = activities, fill = activities, color = activities  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

romantic.plot1 <- ggplot(sacf.dat, aes(y = grades, x = romantic, fill = romantic, color = romantic  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

freetime.plot1 <- ggplot(sacf.dat, aes(y = grades, x = freetime, fill = freetime, color = freetime  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

health.plot1 <- ggplot(sacf.dat, aes(y = grades, x = health, fill = health, color = health  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

goout.plot1 <- ggplot(sacf.dat, aes(y = grades, x = goout, fill = goout, color = goout  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")

reason.plot1 <- ggplot(sacf.dat, aes(y = grades, x = reason, fill = reason, color = reason  ) ) +
  geom_boxplot(alpha = 0.5, bins = 30) +
  theme(legend.position = "right")


plot_grid(sex.plot1, Mjob.plot1, Fjob.plot1, guardian.plot1, famsup.plot1, activities.plot1, romantic.plot1, freetime.plot1, health.plot1, goout.plot1, reason.plot1, 
          labels=c("sex", "Mjob","Fjob", "guardian", "famsup", "activities", "romantic", "freetime", "health", "goout", "reason"), 
          ncol = 3, nrow = 4)


```




```{r paredu plot, cache = TRUE}

sacf.dat <- sacf.dat %>% 
  mutate(paredu.cat = case_when(paredu > 4 ~ "High",
                                       paredu <= 4 ~ "Low",
                                       ))

head(sacf.dat)

ggplot(sacf.dat, aes(x = grades, y = paredu, fill = paredu.cat)) +
  geom_point(size = 2, shape = 16, position = position_jitter(0.5)) +
  facet_wrap(~ paredu.cat)

```




# Check for Near-Zero Variance
```{r nzv, cache = TRUE}

nearZeroVar(
  cat.dat,
  freqCut = 80/20,
  uniqueCut = 10,
  saveMetrics = TRUE,
  names = FALSE,
  foreach = FALSE,
  allowParallel = TRUE
)

#None of the variables failed nzv test. Although this is to check for dichotomized variables 


```

# Selecting final variables to use
```{r selecting new variables, cache = TRUE}
#we remove Mjob, Fjob, and guardian due to skewness issue. 

sac.dat <- select(sacf.dat, sex, age, paredu, famsup, activities, romantic, freetime, health, goout, reason, grades)


head(sac.dat)
str(sac.dat)
summary(sac.dat)
psych::describe(sac.dat)
sum(is.na(sac.dat))


#we kept variables with low skewness but also kept some variables with intermediate skew levels because we are interested in those variables (think they might help predict grades). 

```





# Checking correlations between variables 
```{r Correlations, cache = TRUE}


car::vif(lm(grades ~ ., 
            data = sac.dat,
            ))


#correlations between variables and grades are extremely low here. Where 1 means no correlation and larger number means more correlations
```


# Part 2: Modeling
# Create Train and Test Set
```{r Split, cache = TRUE}

set.seed(1234) 

train.index <- caret::createDataPartition(sac.dat$grades, p = .7, list = FALSE)

train.dat <- sac.dat[ train.index, ]
test.dat <- sac.dat[ - train.index, ]

dim(train.dat)
dim(test.dat)


```




```{r Train Control, cache = TRUE}
my.seed <- 111

set.seed(my.seed)
tr.Control <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5
                           )


```

# Multiple Linear Regression
```{r linear regression, cache = TRUE}

set.seed(my.seed)
lm.fit <- train(grades ~ ., 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

lm.fit$results

```


# Regularized Regression
```{r elastic net, cache = TRUE}

#random search
set.seed(my.seed)
elastic.fit1 <- train(grades ~ ., 
                  data = train.dat, 
                  method = 'glmnet', 
                  trControl = tr.Control,
                  verbose = FALSE,
                  tuneLength = 20,
                  metric = "RMSE",
                  preProcess = c("center", "scale")
                  )

plot(elastic.fit1)
elastic.fit1$results[ rownames(elastic.fit1$bestTune), ]

unique(elastic.fit1$results$alpha) 
#alpha is close to one end of the tuning range.
unique(elastic.fit1$results$lambda)

#grid search
set.seed(my.seed) 
elastic.grid <- expand.grid(alpha = seq(0, 0.3, length.out = 20),
                            lambda = seq(0.1, 0.6, length.out = 20))

set.seed(my.seed)
elastic.fit2 <- train(grades ~ ., 
                      data = train.dat, 
                      method = 'glmnet', 
                      trControl = tr.Control,
                      verbose = FALSE,
                      tuneGrid = elastic.grid,
                      metric = "RMSE",
                      preProcess = c("center", "scale")
                      )

plot(elastic.fit2)
elastic.fit2$results[ rownames(elastic.fit2$bestTune), ]

unique(elastic.fit2$results$alpha) 
unique(elastic.fit2$results$lambda)
#alpha and lambda are not at the end of the tuning range now.


# get the upper and lower RMSE bordering the optimal RMSE
opt.RMSE.index <- which( unique(elastic.fit2$results$RMSE) %in% 
                          elastic.fit2$results[rownames(elastic.fit2$bestTune),])


RMSE.new.range <- unique(elastic.fit2$results$RMSE)[c(opt.RMSE.index -1, 
                                    opt.RMSE.index,
                                    opt.RMSE.index + 1) ]


elastic.fit2$results[elastic.fit2$results$RMSE %in% RMSE.new.range, ]

#how much RMSE varies from one sample to the next = RMSESD/sqrt(50)
cv.sd = elastic.fit2$results [ row.names(elastic.fit2$bestTune),]$RMSESD
cv.n = 50
avCV.SE = cv.sd / sqrt(cv.n)
avCV.SE

#We see that the RMSE has not changed much. The variation in RMSE seems much smaller than the CV RMSE SE = 0.03. So we stop.
#RMSE should vary about 0.03 across different samples. However, the RMSE here vary less than 0.01, so we can stop fine tuning here.











```


# Decision Tree
```{r decision tree, cache = TRUE}

#random search
set.seed(my.seed)
tree.fit1 <- train(grades ~ .,
                   data = train.dat,
                   trControl = tr.Control,
                   method = "rpart",
                   metric = "RMSE",
                   #preProcess = c("center", "scale"),
                   tuneLength = 20
                   )

tree.fit1$results [ row.names(tree.fit1$bestTune),]
plot(tree.fit1)
rpart.plot(tree.fit1$finalModel)

unique(tree.fit1$results$cp)
#cp is quite close to the extreme end of selected range

#grid search
set.seed(my.seed)
tree.fit2 <- train(grades ~ .,
                   data = train.dat,
                   trControl = tr.Control,
                   method = "rpart",
                   metric = "RMSE",
                   #preProcess = c("center", "scale"),
                   tuneGrid = expand.grid(cp = seq(0.014, 0.032, length.out = 20 ) )
                   )

tree.fit2$results [ row.names(tree.fit2$bestTune),]
plot(tree.fit2)
rpart.plot(tree.fit2$finalModel)

unique(tree.fit2$results$cp)
#cp is not close to the end of the tuning range, but the number of buckets is too little and we will fine tune that.


split.vec <- c(20, 40, 50, 60, 70, 80)

# create empty dataframe to store results of all minsplit
tree.result.dat <- data.frame()

for(i in split.vec){
set.seed(my.seed)
tree.fit3 <- train(grades ~ .,
                   data = train.dat,
                   trControl = tr.Control, 
                   method = "rpart",
                   metric = "RMSE",
                   #preProcess = c("center", "scale"),
                   tuneGrid = expand.grid(cp = seq(0.014, 0.032, length.out = 20 ) ),
                   control = rpart.control(minsplit = i)
                 )

temp.result <- tree.fit3$results
temp.result$minsplit = i

tree.result.dat <- rbind(tree.result.dat, temp.result)

}

ggplot(tree.result.dat, aes( x = cp, y = RMSE, color = as.factor(minsplit)) ) +
  geom_point() +
  geom_line()

tree.result.dat %>% 
  filter(RMSE == min(RMSE, na.rm = TRUE))

#we will keep minsplit = 40 and cp = 0.02252632
set.seed(my.seed)

tree.fit4 <- train(grades ~ .,
                   data = train.dat,
                   trControl = tr.Control, 
                   method = "rpart",
                   metric = "RMSE",
                   #preProcess = c("center", "scale"),
                   tuneGrid = expand.grid(cp = 0.02252632	 ),
                   control = rpart.control(minsplit = 40)
                 )

tree.fit4$results [ row.names(tree.fit4$bestTune),]
#plot(tree.fit4)
rpart.plot(tree.fit4$finalModel)




```


# Random Forest
```{r random forest, cache = TRUE}

set.seed(my.seed)
rf.fit1 <- train(grades ~ .,
                 data = train.dat,
                 method = "rf",
                 trControl = tr.Control ,
                 #preProc = c("center", "scale"),  
                 ntree = 1000,
                 metric = "RMSE",
                 tuneGrid = expand.grid(mtry = seq(1, ncol(train.dat)-1))
                 )

#node size here = 10

```


```{r summary rf, cache = TRUE}
# bag.fit

rf.fit1$results
rf.fit1$results[ rownames(rf.fit1$bestTune),]

plot(rf.fit1)
plot.dat <- rf.fit1$results
ggplot(plot.dat, aes(x = mtry, y = Rsquared )) +
  geom_point() +
  geom_line() +
  theme_bw()


#fine tune rf since it looks quite similar to the linear model 

```

```{r node finetune, cache = TRUE}

node.size = c(10, 20, 30, 40, 50, 60)
rf2.results.dat <- NULL 
rf.models <- vector("list", length = length(node.size)) 



for(i in 1: length(node.size) ){
  
  cat("\n Iteration: ",i,  " ; Node size =  ",
      node.size[i], "\n")
  
  set.seed(my.seed)
  
  rf.fit2 <- train(grades ~ ., 
                   data = train.dat, 
                   trControl = tr.Control, 
                   method = "rf",
                 # preProc = c("center", "scale"), 
                   ntree = 1000, 
                   tuneGrid = expand.grid(mtry = seq(1, ncol(train.dat)-1) ), 
                   nodesize = node.size[i]
                   )
  
  rf.models[[i]] <- rf.fit2
  
  results.temp <- rf.fit2$results
  results.temp$node.size = node.size[i]
  
  # print(results.temp)
  
  rf2.results.dat <- rbind(rf2.results.dat, results.temp)
}  



```

```{r summary rf2, cache = TRUE}

rf2plot.dat <- rf2.results.dat
rf2plot.dat$node.size <- factor(rf2plot.dat$node.size)
ggplot(rf2plot.dat, aes(x = mtry, y = RMSE, color = node.size )) +
  geom_point() +
  geom_line() +
  theme_bw()


rf2.results.dat[rf2.results.dat$RMSE == min(rf2.results.dat$RMSE), ]

#from the graph we can see that both node size 10 and 20 does well, almost equally. We will go with node size 10 for our chosen model.
#since there is not a big change from rf.fit1, we will stop fine tuning here.
rf.fitfinal <- rf.models[[ which(node.size == 60)  ]]

```


```{r tree size finetune, cache = TRUE}
tree.N = c(500, 1000, 2000, 3000)

rf3.results.dat <- NULL 
rf.models2 <- vector("list", length = length(tree.N)) 

for(i in 1:length( tree.N)){
set.seed(30825920)
rf.fit3 <- train(grades ~ .,
                 data = train.dat,
                 method = "rf",
                 trControl = tr.Control ,
                 preProc = c("center", "scale"),  
                 ntree = tree.N[i],
                 tuneGrid = expand.grid(mtry = seq(1, ncol(train.dat)-1)
                         )
)

rf.models2[[i]] <- rf.fit3

results.temp <- rf.fit3$results
results.temp$tree.size = tree.N[i]

rf3.results.dat <- rbind(rf3.results.dat, results.temp)
}



```


```{r summary rf3, cache = TRUE}


rf3plot.dat <- rf3.results.dat
rf3plot.dat$tree.size <- factor(rf3plot.dat$tree.size)
ggplot(rf3plot.dat, aes(x = mtry, y = RMSE, color = tree.size )) +
  geom_point() +
  geom_line() +
  theme_bw()


rf3.results.dat[rf3.results.dat$RMSE == min(rf3.results.dat$RMSE), ]

#this did slightly worse than rf.fit2 (otherwise it's the same), so we will use rf.fit2 instead.

```


# Boosting
```{r Boosting, cache = TRUE}

set.seed(my.seed)
gbm.fit1 <- train(grades ~ ., 
                  data = train.dat, 
                  method = "gbm", 
                  trControl = tr.Control, 
                  preProc = c("center", "scale"),
                  tuneLength = 20,
                  verbose = FALSE,
                  metric = "RMSE"
                  )



```


```{r summary boosting, cache = TRUE}

plot(gbm.fit1)

gbm.fit1$results[ rownames(gbm.fit1$bestTune),]


```



```{r boosting finetune, cache = TRUE}

tree.size <- c(50, 100, 500, 1000,  1500, 2000)

gbm.grid <- expand.grid(interaction.depth = c(1, 2, 3, 4),
                        n.trees = tree.size, 
                        shrinkage = c(10^-(1:3)),
                        n.minobsinnode = 10
                        )

set.seed(my.seed)
gbm.fit2 <- train(grades ~ ., 
                 data = train.dat, 
                 method = "gbm", 
                 trControl = tr.Control,  
                 preProc = c("center", "scale"), 
                 tuneGrid = gbm.grid, 
                 verbose = FALSE,
                 metric = "RMSE"
                 )







```



```{r summary gbm2, cache = TRUE}

plot(gbm.fit2)

gbm.fit2$results[ rownames(gbm.fit2$bestTune),]



```


# Part 3: Comparing Models

```{r combine models, fig.height = 10, fig.width= 10, cache = TRUE}

model.resamples <- resamples( list(Regression = lm.fit, 
               Elastic = elastic.fit2, 
               Tree = tree.fit4,
               RF = rf.fitfinal,
               Boosting = gbm.fit2))

summary(model.resamples)
dotplot(model.resamples, metric = "RMSE")
splom(model.resamples, metric = "RMSE")
summary(diff(model.resamples))



```

```{r performance of RMSE across train models, fig.height = 8, fig.width = 10, cache = TRUE}

RMSE.dat <- model.resamples$values %>% 
  select(Resample, ends_with("~RMSE"))

names(RMSE.dat) <- c("folds", "Logistic", "Elastic", "Tree", "RandomForest","GBM")

RMSE.dat %>% 
  pivot_longer(cols = !folds,
               names_to = "Method",
               values_to = "RMSE") %>% 
  
  ggplot(aes(x = Method, y = RMSE, color = Method)) +
  geom_point(position = position_jitter(width = 0.1))


```

```{r performance across cv folds, fig.height = 8, fig.width = 11, cache = TRUE}


RMSE.long <- RMSE.dat %>% 
  pivot_longer(cols = !folds,
               names_to = "Method",
               values_to = "RMSE") %>%  
  separate(folds, into =  c("Fold", "Rep"))


RMSE.long %>% 
   ggplot(aes(x = Fold, y = RMSE, color = Method)) +
  geom_point(position = position_dodge(width = 0.3), alpha = 0.3) +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), 
               geom = "pointrange", position = position_dodge(width = 0.5)) +
  theme_bw()
```


```{r performance across repetitions, fig.height = 8, fig.width = 10, cache = TRUE}
RMSE.long %>% 
   ggplot(aes(x = Method, y = RMSE, color = Rep)) +
  geom_point(position = position_dodge(width = 0.3), alpha = 0.3) +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), 
               geom = "pointrange", position = position_dodge(width = 0.5)) +
  theme_bw()
```

```{r cv measure for best tune, cache = TRUE}
# getting cv measure for best tune :
cv.dat <- rbind(lm.fit$results[rownames(lm.fit$bestTune), c("RMSE", "RMSESD") ],
      elastic.fit2$results[rownames(elastic.fit2$bestTune), c("RMSE", "RMSESD") ],
      tree.fit4$results[rownames(tree.fit4$bestTune), c("RMSE", "RMSESD") ],
      rf.fitfinal$results[rownames(rf.fitfinal$bestTune), c("RMSE", "RMSESD")],
      gbm.fit2$results[rownames(gbm.fit2$bestTune), c("RMSE", "RMSESD")]
      )

# getting model names from resamples()
cv.dat$model <- model.resamples$models

#the number of repeated cv folds = 10 x 5 = 50
cv.N = 50

#SE = sd / sqrt(length(x))

#table:
cv.dat %>% 
  mutate(SE = RMSESD / sqrt(cv.N))

```


```{r plot RMSE over model, fig.height = 8, fig.width = 10, cache = TRUE}

# plot: 
cv.dat %>% 
  mutate(SE = RMSESD / sqrt(cv.N)) %>% 

ggplot(aes(x = model, y = RMSE, color = model)) +
  # 95%CI for CV average RMSE
  geom_pointrange(aes(ymin = RMSE - 1.96*SE, ymax = RMSE + 1.96*SE) )

```


# Check Variable Importance
```{r vip train, fig.height = 8, fig.width= 18, cache = TRUE}

p1 <- vip(lm.fit) + ggtitle("Regression")
p2 <- vip(elastic.fit2) + ggtitle("Elastic")
p3 <- vip(tree.fit4) + ggtitle("Tree")
p4 <- vip(rf.fitfinal) + ggtitle("RF")
p5 <- vip(gbm.fit2) + ggtitle("Boosting")

grid.arrange(p1, p2, p3, p4, p5, ncol = 5)

#combined top 3 from all:
  ##paredu
  ##sex
  #goout
  ##reason
  ##age

```


# Part 4: Model Performance
```{r Predict, cache = TRUE}



#Getting RMSE and Rsquared for each model on test.dat
#linear regression
lm.pred <- predict(lm.fit, newdata = test.dat, type = "raw")
lm.RMSE.pred <- RMSE(lm.pred, test.dat$grades)
lm.R2.pred <- R2(lm.pred, test.dat$grades)

#elastic net
elastic.pred <- predict(elastic.fit2, newdata = test.dat, type = "raw")
elastic.RMSE.pred <- RMSE(elastic.pred, test.dat$grades)
elastic.R2.pred <- R2(elastic.pred, test.dat$grades)

#decision tree
tree.pred <- predict(tree.fit4, newdata = test.dat, type = "raw")
tree.RMSE.pred <- RMSE(tree.pred, test.dat$grades)
tree.R2.pred <- R2(tree.pred, test.dat$grades)

#random forest
rf.pred <- predict(rf.fitfinal, newdata = test.dat, type = "raw")
rf.RMSE.pred <- RMSE(rf.pred, test.dat$grades)
rf.R2.pred <- R2(rf.pred, test.dat$grades)

#boosting
gbm.pred <- predict(gbm.fit2, newdata = test.dat, type = "raw")
gbm.RMSE.pred <- RMSE(gbm.pred, test.dat$grades)
gbm.R2.pred <- R2(gbm.pred, test.dat$grades)



test.results<- data.frame(models=c("lm","elastic","tree","rf","gbm"),
                          train.RMSE=c(lm.fit$results$RMSE,
                                       elastic.fit2$results[ rownames(elastic.fit2$bestTune), ]$RMSE,
                                       tree.fit4$results [ row.names(tree.fit4$bestTune),]$RMSE,
                                       rf2.results.dat[rf2.results.dat$RMSE == min(rf2.results.dat$RMSE), ]$RMSE,
                                       gbm.fit2$results[ rownames(gbm.fit2$bestTune),]$RMSE),
                          train.R2=c(lm.fit$results$Rsquared,
                                     elastic.fit2$results[ rownames(elastic.fit2$bestTune), ]$Rsquared,
                                     tree.fit4$results [ row.names(tree.fit4$bestTune),]$Rsquared,
                                     rf2.results.dat[rf2.results.dat$RMSE == min(rf2.results.dat$RMSE), ]$Rsquared,
                                     gbm.fit2$results[ rownames(gbm.fit2$bestTune),]$Rsquared
),
                          test.RMSE=c(lm.RMSE.pred,elastic.RMSE.pred,tree.RMSE.pred,rf.RMSE.pred,gbm.RMSE.pred),
                         test.R2=c(lm.R2.pred,elastic.R2.pred,tree.R2.pred,rf.R2.pred,gbm.R2.pred))
print(test.results)

#minimum RMSE for train set
test.results[which.min(test.results$train.RMSE),]

#minimum RMSE for test set
test.results[which.min(test.results$test.RMSE),]

#based on the results, we can see that elastic had the lowest RMSE and highest Rsquared in train data. However, gbm had the lowest RMSE and higher Rsquared in test data.  
```


# Bootstrapping
```{r dealing with testset uncertainty:bootstrapping, cache = TRUE}
boot.iter = 1000
set.seed(my.seed)

boot.dat <- data.frame(LM= vector(length = boot.iter),
                       Elastic = vector(length = boot.iter),
                       Tree = vector(length = boot.iter),
                       RF =  vector(length = boot.iter),
                       GBM = vector(length = boot.iter)
           )

for ( i in 1: boot.iter){
  
  # get a bootstrap sample:
  boot.index <- sample(   nrow(test.dat), replace = TRUE   )
  boot.sample <- test.dat[ boot.index, ]
  
  pred.dat <- data.frame( y.lm = predict(lm.fit, newdata = boot.sample),
                          y.elastic = predict(elastic.fit2, newdata = boot.sample),
                          y.tree= predict(tree.fit4, newdata = boot.sample),
                          y.rf = predict(rf.fitfinal, newdata = boot.sample),
                          y.gbm = predict(gbm.fit2, newdata = boot.sample))
  
  
  sq.err.dat <- (boot.sample$grades - pred.dat)^2 
  
  boot.MSE = colMeans(sq.err.dat)
  
  boot.RMSE = sqrt(boot.MSE)
  boot.dat[i, ] <-    boot.RMSE
  
}

head(boot.dat)
```

```{r tibble, cache = TRUE}
mean.dat <- boot.dat %>% 
  pivot_longer(cols = everything(),
               names_to = "Model",
               values_to = "RMSE") %>% 
  group_by(Model) %>% 
  summarize(mean = mean(RMSE))

mean.dat

```

```{r bootstrap histogram, fig.height = 8, fig.width= 18, cache = TRUE}
boot.dat %>% 
  pivot_longer(cols = everything(),
               names_to = "Model",
               values_to = "boot.RMSE") %>% 
  
  ggplot(aes(x = boot.RMSE, fill = Model)) +
  geom_histogram(col = "grey") +
  geom_vline(data = mean.dat, aes(xintercept = mean), col = "black", lty = 2 ) +
  facet_wrap( ~ Model, ncol = 1)
```

```{r bootstrap ci, cache = TRUE}
# parametric bootstrap 95%CI: assumes normal distribution:
par.ci.dat <- data.frame(RMSE =apply(boot.dat, 2, mean, na.rm = TRUE),
                       SD = apply(boot.dat, 2, sd, na.rm = TRUE),
                error.type = "Test-bootstrap",
                model.type = c("LM","Elastic", "Tree","RF","GBM")
                      ) %>%
  group_by(model.type) %>%
  summarize( RMSE = RMSE, 
             lower = RMSE - 1.96*SD, 
             upper = RMSE + 1.96*SD,
            error.type = "Test.boot.param" )


# empirical 95%CI: percentile bootstrap CI
plyr::ldply(boot.dat, quantile, c(.025, .975))
```

```{r mean RMSE across models for test.dat, cache = TRUE}
par.ci.dat %>% 
  ggplot(aes(x = model.type, y = RMSE, color = model.type)) +
  geom_pointrange(aes(ymin = lower, 
                      ymax = upper),
                  position = position_dodge(width = 0.2))
```


```{r plot test-set error and error bars from parametric bootstrap SEs, cache = TRUE}
test.ci.dat <- data.frame(RMSE = test.results$test.RMSE,
                       SD = apply(boot.dat, 2, sd, na.rm = TRUE),
                error.type = "Test-bootstrap",
                model.type = c("LM","Elastic", "Tree","RF","GBM")
                      ) %>%
  group_by(model.type) %>%
  summarize( RMSE = RMSE, 
             lower = RMSE - 1.96*SD, 
             upper = RMSE + 1.96*SD,
            error.type = "Test.boot.param" )

test.ci.dat %>% 
  ggplot(aes(x = model.type, y = RMSE, color = model.type)) +
  geom_pointrange(aes(ymin = lower, 
                      ymax = upper),
                  position = position_dodge(width = 0.2))

```

```{r comparision between bootstraps & test error, cache = TRUE}
print(par.ci.dat)
print(test.ci.dat)
```


# Partial-Dependence Plots
```{r PDP1, fig.height = 10, fig.width = 10, cache = TRUE}

#combined top 3 from all:
  ##paredu
  ##sex
  #goout
  ##reason
  ##age

#first we try paredu, age, sex 
pdp.lm1 <- partial(lm.fit, pred.var = c("paredu", "age", "sex"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.elastic1 <- partial(elastic.fit2, pred.var = c("paredu", "age", "sex"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.tree1 <- partial(tree.fit4, pred.var = c("paredu", "age", "sex"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.rf1 <- partial(rf.fitfinal, pred.var = c("paredu", "age", "sex"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)



pdp.gbm1 <- partial(gbm.fit2, pred.var = c("paredu", "age", "sex"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)




lm.plot1 <- plotPartial(pdp.lm1, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Linear Regression"
            )

elastic.plot1 <- plotPartial(pdp.elastic1, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Regularised Regression"
            )

tree.plot1 <- plotPartial(pdp.tree1, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Decision Tree"
            )

rf.plot1 <- plotPartial(pdp.rf1, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
            main = "Random Forest"
            )

gbm.plot1 <- plotPartial(pdp.gbm1, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Gradient Boosting"
            )





grid.arrange(lm.plot1, elastic.plot1, tree.plot1, rf.plot1, gbm.plot1, ncol = 2, nrow = 3)



```

```{r PDP2, fig.height = 15, fig.width = 15, cache = TRUE}

#now we try paredu, age, reason
pdp.lm2 <- partial(lm.fit, pred.var = c("paredu", "age", "reason"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.elastic2 <- partial(elastic.fit2, pred.var = c("paredu", "age", "reason"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.tree2 <- partial(tree.fit4, pred.var = c("paredu", "age", "reason"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.rf2 <- partial(rf.fitfinal, pred.var = c("paredu", "age", "reason"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.gbm2 <- partial(gbm.fit2, pred.var = c("paredu", "age", "reason"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)




lm.plot2 <- plotPartial(pdp.lm2, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Linear Regression"
            )

elastic.plot2 <- plotPartial(pdp.elastic2, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Regularised Regression"
            )

tree.plot2 <- plotPartial(pdp.tree2, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Decision Tree"
            )

rf.plot2 <- plotPartial(pdp.rf2, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
            main = "Random Forest"
            )

gbm.plot2 <- plotPartial(pdp.gbm2, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Gradient Boosting"
            )


grid.arrange(lm.plot2, elastic.plot2, tree.plot2, rf.plot2, gbm.plot2, ncol = 2, nrow = 3)

#there are hints of interaction across reason in decision trees
```


```{r PDP3, fig.height = 15, fig.width = 15, cache = TRUE}

#lastly we try paredu, age, goout
pdp.lm3 <- partial(lm.fit, pred.var = c("paredu", "age", "goout"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.elastic3 <- partial(elastic.fit2, pred.var = c("paredu", "age", "goout"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.tree3 <- partial(tree.fit4, pred.var = c("paredu", "age", "goout"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)

pdp.rf3 <- partial(rf.fitfinal, pred.var = c("paredu", "age", "goout"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)



pdp.gbm3 <- partial(gbm.fit2, pred.var = c("paredu", "age", "goout"),
              plot.engine = "ggplot",
              train = train.dat,
              chull = FALSE)




lm.plot3 <- plotPartial(pdp.lm3, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Linear Regression"
            )

elastic.plot3 <- plotPartial(pdp.elastic3, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Regularised Regression"
            )

tree.plot3 <- plotPartial(pdp.tree3, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Decision Tree"
            )

rf.plot3 <- plotPartial(pdp.rf3, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
            main = "Random Forest"
            )

gbm.plot3 <- plotPartial(pdp.gbm3, levelplot = FALSE, zlab = "grades", drape = TRUE,
            colorkey = TRUE, 
            # screen = list(z = -20, x = -80),
             main = "Gradient Boosting"
            )





grid.arrange(lm.plot3, elastic.plot3, tree.plot3, rf.plot3, gbm.plot3, ncol = 2, nrow = 3)
```

```{r PDP4, cache = TRUE}
ggplot(pdp.tree2, aes(x = paredu, y = yhat, color = age)) +
  geom_line(aes(group = as.factor(age) ) ) +
  facet_wrap(~reason)
```


```{r PDP5, cache = TRUE}

ggplot(pdp.gbm1, aes(x = paredu, y = yhat, color = age)) +
  geom_line(aes(group = as.factor(age) ) ) +
  facet_wrap(~sex)


```


# Additive Effects
```{r complex linear regression additive, cache = TRUE}

set.seed(my.seed)
lm.add1 <- train(grades ~ . + I(paredu^2), 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )



set.seed(my.seed)
lm.add2 <- train(grades ~ . + I(paredu^2) + I(paredu^3), 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )



set.seed(my.seed)
lm.add3 <- train(grades ~ . + I(paredu^2) + I(paredu^3) + I(paredu^4), 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )



set.seed(my.seed)
lm.add4 <- train(grades ~ . + I(age^2), 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )


set.seed(my.seed)
lm.add5 <- train(grades ~ . + I(age^2) + I(age^3), 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )


set.seed(my.seed)
lm.add6 <- train(grades ~ . + I(age^2) + I(age^3) +  I(age^4), 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

lm.addresults <- rbind(lm.fit$results, lm.add1$results, lm.add2$results, lm.add3$results, lm.add4$results, lm.add5$results, lm.add6$results)
print(lm.addresults)

lm.addresults[which.min(lm.addresults$RMSE),]

#we see that lm.add4, with addititive effect of age helped reduce RMSE by a little and increase Rsquared by a little as well. Overall, not a lot of effect. 

```


# Interaction effects
```{r complex linear regression interaction,cache = TRUE}

#no interactions were shown in PDPs but we will try anyway.
set.seed(my.seed)
lm.int1 <- train(grades ~ . + age:reason, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int2 <- train(grades ~ . + I(age^2) + age:reason, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )



set.seed(my.seed)
lm.int3 <- train(grades ~ . + I(age^2) + I(age^3) + age:reason, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int4 <- train(grades ~ . + age:sex, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int5 <- train(grades ~ . + I(age^2) + age:sex, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int6 <- train(grades ~ . + I(age^2) + I(age^3) + age:sex, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )
set.seed(my.seed)
lm.int7 <- train(grades ~ . + age:paredu, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int8 <- train(grades ~ . + I(age^2) + age:paredu, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int9 <- train(grades ~ . + I(age^2) + I(age^3) + age:paredu, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )
set.seed(my.seed)
lm.int10 <- train(grades ~ . + age:goout, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int11 <- train(grades ~ . + I(age^2) + age:goout, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )

set.seed(my.seed)
lm.int12 <- train(grades ~ . + I(age^2) + I(age^3) + age:goout, 
             method = "lm",
             trControl = tr.Control,
             data = train.dat,
             metric = "RMSE",
             preProcess = c("center", "scale")
             )




lm.intresults <- rbind(lm.fit$results, lm.int1$results, lm.int2$results, lm.int3$results, lm.int4$results, lm.int5$results, lm.int6$results, lm.int7$results, lm.int8$results, lm.int9$results, lm.int10$results, lm.int11$results, lm.int12$results )
print(lm.intresults)

lm.intresults[which.min(lm.intresults$RMSE),]
#we see that lm.int5 with additive age and interaction between age and sex helped decrease RMSE and increase Rsquared by a little, but still not so much. This is also higher than the RMSE of the additive model from before.  
#since the improvement is minimal, we will not use it for further analyses. 

```






```{r predictions & residuals, cache = TRUE}

# get predictions
res.dat <- data.frame(test.y = test.dat$grades,
                      lm.pred, 
                      elastic.pred,
                      tree.pred,
                      rf.pred, 
                      gbm.pred)


# get residuals
res.dat <- res.dat %>% 
            mutate(
            resid.lm = test.y - lm.pred,
            resid.elastic = test.y - elastic.pred,
            resid.tree= test.y - tree.pred,
            resid.rf = test.y - rf.pred,
            resid.gbm = test.y - gbm.pred
            )


```







```{r long format residual,cache = TRUE}

#transform to long format 
#first get a residuals only dataset, then merge with a predictions only dataset:

# 1. get residuals only data:
residuals.long <- res.dat %>% 
  select(resid.lm, resid.elastic, resid.tree, resid.rf, resid.gbm, test.y) %>% 
  pivot_longer(cols = !test.y, 
               names_to = "method", 
               values_to = "residuals")

#clean up the labels in "method"
residuals.long <- 
  residuals.long %>% 
  mutate(method = case_when(method == "resid.lm" ~ "Linear",
                            method == "resid.elastic" ~ "Elastic",
                            method == "resid.tree" ~ "Tree",
                            method == "resid.rf" ~ "RF",
                            method == "resid.gbm" ~ "GBM")
         )%>% 
  mutate(ID = 1:nrow(residuals.long))

# 2.get predictions-only data:
predictions.long <- res.dat %>% 
  select(lm.pred, elastic.pred, tree.pred, rf.pred,gbm.pred,test.y) %>% 
  pivot_longer(cols = !test.y, 
               names_to = "method", 
               values_to = "predictions")

#clean up the labels in "method"
predictions.long <- 
  predictions.long %>% 
  mutate(method = case_when(method == "lm.pred" ~ "Linear",
                            method == "elastic.pred" ~ "Elastic",
                            method == "tree.pred" ~ "Tree",
                            method == "rf.pred" ~ "RF",
                            method == "gbm.pred" ~ "GBM")
         ) %>% 
  mutate(ID = 1:nrow(predictions.long))

# 3. merge both to get the diagnostics dataset:
diag.dat <- merge(predictions.long, residuals.long)
head(diag.dat)

```


```{r plot residuals vs predictions, cache = TRUE}

ggplot(diag.dat, aes(x = predictions, y = residuals)) +
  geom_point(alpha = 0.2) +
  geom_hline(yintercept = 0, col = "red", lty = 2) +
  geom_smooth(se = FALSE, lty = 1) +
  facet_wrap(~method) +
  theme_minimal()

```




```{r plotpredictions vs test data, fig.height = 8, fig.width = 10, cache = TRUE}
ggplot(diag.dat, aes(x = test.y, y = predictions)) +
  geom_point(alpha = 0.2) +
  geom_abline(intercept = 0, slope = 1, col = "red", lty = 2) +
  geom_smooth(se = FALSE, lty = 1) +
  facet_wrap(~method) +
  theme_minimal()
```



```{r variable importance using permutations- linear, cache = TRUE}
dv = "grades"
pred.vars <- names(sac.dat)[!names(sac.dat)%in% dv]
explainer.dat <- test.dat

# build explainer for linear regression:
lm.exp <- DALEX::explain(model = lm.fit, 
                  data = explainer.dat[ , pred.vars],
                  y = explainer.dat[ , dv], 
                  label = "Linear Regression",
                  type = "regression")
```



```{r elastic explainer, cache = TRUE}
# build explainer for elastic net:
elastic.exp <- DALEX::explain(model = elastic.fit2, 
                       data = explainer.dat[ , pred.vars], 
                       y = explainer.dat[, dv], 
                       label = "Elastic",
                       type = "regression")
```



```{r tree explainer, cache = TRUE}
# build explainer for tree:
tree.exp <- DALEX::explain(model = tree.fit4, 
                    data = explainer.dat[ , pred.vars], 
                    y = explainer.dat[, dv], 
                    label = "Tree",
                    type = "regression")
```


```{r rf explainer, cache = TRUE}
# build explainer for RF:
rf.exp <- DALEX::explain(model = rf.fitfinal, 
                  data = explainer.dat[ , pred.vars], 
                  y = explainer.dat[, dv], 
                  label = "Random Forest",
                  type = "regression")
```


```{r gbm explainer, cache = TRUE}
# build explainer for GBM:
gbm.exp <- DALEX::explain(model = gbm.fit2, 
                   data = explainer.dat[ , pred.vars], 
                   y= explainer.dat[, dv], 
                   label = "GBM",
                   type = "regression")
```


```{r model part, cache = TRUE}
set.seed(my.seed)
vip_lm  <- model_parts(explainer = lm.exp,  B = 50, N = NULL)
set.seed(my.seed)
vip_elastic  <- model_parts(explainer = elastic.exp,  B = 50, N = NULL)
set.seed(my.seed)
vip_tree  <- model_parts(explainer = tree.exp,  B = 50, N = NULL)
set.seed(my.seed)
vip_rf  <- model_parts(explainer = rf.exp,  B = 50, N = NULL)
set.seed(my.seed)
vip_gbm <- model_parts(explainer = gbm.exp, B = 50, N = NULL)
```


```{r, fig.height = 25, fig.width = 15, cache = TRUE}

plot(vip_lm, vip_elastic,vip_tree, vip_rf, vip_gbm) +
  ggtitle("Mean variable-importance over 50 permutations", "") 
```



# Create Partial-Dependence Profiles
```{r PDP for age, cache = TRUE}

pdp_lm.age <- model_profile(explainer = lm.exp, variables = "age")
pdp_elastic.age <- model_profile(explainer = elastic.exp, variables = "age")
pdp_tree.age <- model_profile(explainer = tree.exp, variables = "age")
pdp_rf.age <- model_profile(explainer = rf.exp, variables = "age")
pdp_gbm.age <- model_profile(explainer = gbm.exp, variables = "age")


plot(pdp_lm.age, pdp_elastic.age, pdp_tree.age, pdp_rf.age, pdp_gbm.age) +  
  ggtitle("Partial-dependence profile for age") 


```



```{r PDP for paredu, cache = TRUE}
pdp_lm.paredu <- model_profile(explainer = lm.exp, variables = "paredu")
pdp_elastic.paredu <- model_profile(explainer = elastic.exp, variables = "paredu")
pdp_tree.paredu <- model_profile(explainer = tree.exp, variables = "paredu")
pdp_rf.paredu <- model_profile(explainer = rf.exp, variables = "paredu")
pdp_gbm.paredu <- model_profile(explainer = gbm.exp, variables = "paredu")


plot(pdp_lm.paredu, pdp_elastic.paredu, pdp_tree.paredu, pdp_rf.paredu, pdp_gbm.paredu) +  
  ggtitle("Partial-dependence profile for paredu") 
```



# Create Ceteris-Paribus Profiles
```{r cp for paredu, cache = TRUE}
plot(pdp_lm.paredu, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for paredu (lm)")
plot(pdp_elastic.paredu, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for paredu (elastic)")
plot(pdp_tree.paredu, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for paredu (tree)")
plot(pdp_rf.paredu, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for paredu (rf)") 
plot(pdp_gbm.paredu, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for paredu (gbm)") 
 
 
 
```




```{r cp for age, cache = TRUE}

plot(pdp_lm.age, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for age (lm)") 
plot(pdp_elastic.age, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for age (elastic)")
plot(pdp_tree.age, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for age (tree)") 
plot(pdp_rf.age, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for age (rf)") 
plot(pdp_gbm.age, geom = "profiles") + 
    ggtitle("Ceteris-paribus and partial-dependence profiles for age (gbm)")


```



# Part 5: Model Prediction Test
```{r instance level, cache = TRUE}

nesh.dat <- data.frame(
  sex = "Male",
  age = 17,
  paredu = 7,
  famsup = "yes",
  activities = "no",
  romantic = "yes",
  freetime = "1",
  health = "4",
  goout = "1",
  reason = "home"
  )


predict(lm.fit, newdata = nesh.dat)
predict(elastic.fit2, newdata = nesh.dat)
predict(tree.fit4, newdata = nesh.dat)
predict(rf.fitfinal, newdata = nesh.dat)
predict(gbm.fit2, newdata = nesh.dat)







```


```{r cp profiles, cache = TRUE}

cp.lm <- predict_profile(explainer = lm.exp, 
                           new_observation = nesh.dat)


cp.elastic <- predict_profile(explainer = elastic.exp, 
                           new_observation = nesh.dat)

cp.tree <- predict_profile(explainer = tree.exp, 
                           new_observation = nesh.dat)

cp.rf <- predict_profile(explainer = rf.exp, 
                           new_observation = nesh.dat)

cp.gbm <- predict_profile(explainer = gbm.exp, 
                           new_observation = nesh.dat)
cp.lm
cp.elastic
cp.tree
cp.rf
cp.gbm



```



```{r cp profiles plot,cache = TRUE}

plot(cp.lm, variables = c("age", "paredu")) +
  ggtitle("Ceteris-paribus profile", "")

plot(cp.elastic, variables = c("age", "paredu")) +
  ggtitle("Ceteris-paribus profile", "")

plot(cp.tree, variables = c("age", "paredu")) +
  ggtitle("Ceteris-paribus profile", "")

plot(cp.rf, variables = c("age", "paredu")) +
  ggtitle("Ceteris-paribus profile", "")

plot(cp.gbm, variables = c("age", "paredu")) +
  ggtitle("Ceteris-paribus profile", "")

```



```{r combined cp profiles plot, cache = TRUE}

plot(cp.lm, cp.elastic, cp.tree, cp.rf, cp.gbm,color = "_label_",  
     variables = c("age", "paredu")) +
     ggtitle("Ceteris-paribus profiles for Nesh", "") 

```


```{r compare individual, cache = TRUE}

nesh.dat.1 <- data.frame(
  sex = "Male",
  age = 17,
  paredu = 7,
  famsup = "yes",
  activities = "no",
  romantic = "yes",
  freetime = "1",
  health = "4",
  goout = "1",
  reason = "home"
  )

nesh.dat.2 <- data.frame(
  sex = "Male",
  age = 17,
  paredu = 7,
  famsup = "yes",
  activities = "no",
  romantic = "yes",
  freetime = "1",
  health = "4",
  goout = "1",
  reason = "reputation"
  )




cp.elastic1 <- predict_profile(explainer = elastic.exp, 
                         new_observation = rbind(nesh.dat.1,
                                                 nesh.dat.2))
plot(cp.elastic1 , color = "_ids_",
     variables = c("age", "paredu")) +
    scale_color_manual(name = "Nesh:", breaks = 1:2, 
            values = c("red", "blue"), 
            labels = c("home" , "reputation")) +
  ggtitle("Ceteris-paribus profile elastic", "")





cp.gbm1 <- predict_profile(explainer = gbm.exp, 
                         new_observation = rbind(nesh.dat.1,
                                                 nesh.dat.2))
plot(cp.gbm1 , color = "_ids_",
     variables = c("age", "paredu")) +
    scale_color_manual(name = "Nesh:", breaks = 1:2, 
            values = c("red", "blue"), 
            labels = c("home" , "reputation")) +
  ggtitle("Ceteris-paribus profile gbm", "")


```



```{r breakdown plots elastic, cache = TRUE}
bd.elastic <- predict_parts(explainer = elastic.exp, 
                       new_observation = nesh.dat,
                       type = "break_down")

bd.elastic
plot(bd.elastic)




```


```{r breakdown plots gbm, cache = TRUE}

bd.gbm<- predict_parts(explainer = gbm.exp, 
                       new_observation = nesh.dat,
                       type = "break_down")

bd.gbm
plot(bd.gbm)



```





